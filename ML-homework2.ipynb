{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a5044694-9060-4d7f-b9da-62ab5db52ee2",
      "cell_type": "code",
      "source": "# Problem 6\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\nX = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])\ny = np.array([[0], [1], [1], [0]])\nnp.random.seed(42)\n\ninput_layer_neurons = 2\nhidden_layer_neurons = 2\noutput_layer_neurons = 1 \n\nweights_input_hidden = np.random.randn(input_layer_neurons, hidden_layer_neurons)\nbias_hidden = np.random.randn(1, hidden_layer_neurons)\n\nweights_hidden_output = np.random.randn(hidden_layer_neurons, output_layer_neurons)\nbias_output = np.random.randn(1, output_layer_neurons)\n\nepochs = 10000\nlearning_rate = 0.1\n\nfor epoch in range(epochs):\n    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n    hidden_layer_output = sigmoid(hidden_layer_input)\n\n    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n    output_layer_output = sigmoid(output_layer_input)\n    \n    error = y - output_layer_output\n    \n    d_output = error * sigmoid_derivative(output_layer_output)\n    \n    d_hidden_layer = d_output.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output)\n    weights_hidden_output += hidden_layer_output.T.dot(d_output) * learning_rate\n    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n    \n    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n\n    if epoch % 1000 == 0:\n        loss = np.mean(np.square(error))\n        print(f\"Epoch {epoch}, Loss: {loss}\")\n\nprint(\"Final output after training:\")\nhidden_layer_output = sigmoid(np.dot(X, weights_input_hidden) + bias_hidden)\noutput_layer_output = sigmoid(np.dot(hidden_layer_output, weights_hidden_output) + bias_output)\nprint(output_layer_output)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 0, Loss: 0.2943508288500037\nEpoch 1000, Loss: 0.24440977094692865\nEpoch 2000, Loss: 0.20353909817812016\nEpoch 3000, Loss: 0.1533485406663887\nEpoch 4000, Loss: 0.04633103600452313\nEpoch 5000, Loss: 0.015614674284243913\nEpoch 6000, Loss: 0.008448376076782201\nEpoch 7000, Loss: 0.0056135135577574805\nEpoch 8000, Loss: 0.004146485528192295\nEpoch 9000, Loss: 0.0032635336784319563\nFinal output after training:\n[[0.05395132]\n [0.9505447 ]\n [0.95009809]\n [0.05355567]]\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "dc58fcdb-372f-4f89-b7ca-c101197e5985",
      "cell_type": "code",
      "source": "# Problem 9\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\nx_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\ncnn_model = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\ncnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\ncnn_history = cnn_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "226c8d88-84c2-4a55-af09-ae68fb06b20e",
      "cell_type": "code",
      "source": "#DNN\n\ndnn_model = models.Sequential([\n    layers.Flatten(input_shape=(28, 28)),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\ndnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\ndnn_history = dnn_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31885465-5499-42e6-8485-3edc61566651",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\nplt.plot(cnn_history.history['accuracy'], label='CNN Training Accuracy')\nplt.plot(cnn_history.history['val_accuracy'], label='CNN Validation Accuracy')\nplt.plot(dnn_history.history['accuracy'], label='DNN Training Accuracy')\nplt.plot(dnn_history.history['val_accuracy'], label='DNN Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training & validation loss\nplt.plot(cnn_history.history['loss'], label='CNN Training Loss')\nplt.plot(cnn_history.history['val_loss'], label='CNN Validation Loss')\nplt.plot(dnn_history.history['loss'], label='DNN Training Loss')\nplt.plot(dnn_history.history['val_loss'], label='DNN Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}